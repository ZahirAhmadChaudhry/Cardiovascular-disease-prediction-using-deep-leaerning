# -*- coding: utf-8 -*-
"""IDMCAD (images).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14d-id2JMOXOf1Qi8lJFpVyY323Rgvr5X
"""

## Importing Some Useful Initial Libraries
import tensorflow as tf
from tensorflow.keras import models, layers
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

# Declaraing Some CONSTANTS for future use
imageSize = 299 # Defualt images size (width*height)
batchSize = 32 # To import the images in the groub of batches
CHANNEL = 3 # Defuatl images are in RGB (Three-Channel)

# Importing the Training Data set, placed in GDRIVE.
dataset = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/CAD_images_dataset",
    shuffle=True, # to shuffle the places of images every time, when dataset loads
    image_size = (imageSize,imageSize),
    batch_size = batchSize
)

# To view the Classes in the datasets
class_names = dataset.class_names
class_names

# To view the number of images in a Single Batch
len(dataset)

# To view the Images Size and channel: (width, heigh, channel: 1 for grayscale, 3 for RGB)
for image_batch, label_batch in dataset.take(1):
  print(image_batch[0].shape)

plt.figure(figsize=(10,10)) # Defining the Figure size in which each image will be shown
for image_batch, label_batch in dataset.take(1):
  for i in range(12): # Definin the number of images that we want to show
    plt.subplot(3,4, i+1) # creating the subplot (matrix) so that each image could be shown as a one elemet of matrix
    plt.imshow(image_batch[0].numpy().astype("uint8"))
    plt.title(class_names[label_batch[0]])
    plt.axis("off") # hides the axis number

# Splitting dataset into training, testing and validation
# 80% training
# 10 Testing
# 10 Validation

# To check how many numbers of batches are equal to 80% of dataset
train_size = 0.8
len(dataset)*train_size

# Declaring Training Dataset 
train_ds = dataset.take(150)
len(train_ds)

# Declaring Test Dataset
test_ds = dataset.skip(150) # Skipping first 116 batches from the dataset and saving remaining as test dataset
len(test_ds)

# Declaring validation data set
val_ds = test_ds.skip(19)
len(val_ds)

# Optimizing datasets for training performance,
# used Cache to prevent the repetative uploading of same image,
# used prefetch to enhance the training speed, by using prefetch,
# CPU first read the dataset batch--> pas it to GPU for training-->and CPU will starts reading next batch, so on
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

# Creating a constant variable
# Resizing the all images to the same scale
# Rescalling the all images to binary (between 0 and 1)
resize_and_rescale = tf.keras.Sequential([
                     layers.experimental.preprocessing.Resizing(imageSize, imageSize),
                     layers.experimental.preprocessing.Rescaling(1.0/255)
])

# Data augmentation is creating different images from the same image
# by flipping, zooming, rotating etc
# To achieve higher accuracy and verstile model that could predict the zoomed, rotated images
# To increase the number of images to that trainig model could be feed well enough for training
data_augmentation = tf.keras.Sequential([
                     layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
                     layers.experimental.preprocessing.RandomRotation(0.2)
])

#Creating Model
input_shape = (CHANNEL, imageSize, imageSize, CHANNEL)
n_classes = 3
model = models.Sequential([
                           #Image Resize and Rescale layer
                           resize_and_rescale,

                           #Data Augmentation Layer
                           data_augmentation,

                           #Step - 1 : Convolutional
                           layers.Conv2D(32, (3,3), activation = 'relu', input_shape = input_shape),
                           
                           #Step - 2 : Pooling
                           layers.MaxPool2D((2,2)),

                           #Adding a second convulutional layer
                           layers.Conv2D(64, kernel_size=(3,3), activation = 'relu'),
                           layers.MaxPool2D((2,2)),

                           #Adding a third convolutional layer
                           layers.Conv2D(128, kernel_size=(3,3), activation = 'relu'),
                           layers.MaxPool2D((2,2)),

                           #Adding a fourth convolutional layer
                           layers.Conv2D(128, (3,3), activation = 'relu'),
                           # 10th max pooling layer
                           layers.MaxPool2D((2,2)),

                           #Step - 3 : Flattening
                           layers.Flatten(),

                           # Step - 4 : Full Connection
                           layers.Dense(64, activation ='relu'),
                           layers.Dense(n_classes, activation = 'softmax')
])

model.build(input_shape = input_shape)

# To check the summary of the built model

model.summary()

# Compiling the model

model.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

## Early Stopping: when the accuracy of the model is not increasing, training of the model will automatically stop.
import tensorflow as tf
early_stop = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.0001, ## Minimum change in the monitored quantity to qualify as an improvement
    patience=10, ## Number of epochs with no improvement after which training will be stopped.
    verbose=1, ## Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1 displays messages when the callback takes an action.
    mode="auto", 
    baseline=None,
    restore_best_weights=False,
)

from gc import callbacks
# Training the model

history = model.fit(
    train_ds,
    epochs = 1000,
    batch_size = batchSize,
    verbose = 1,
    validation_data = val_ds,
    callbacks= early_stop
)

# Evaluating the model
scores = model.evaluate(test_ds)

scores

"""## Saving the Model"""

model.save('img_mdl.h5')