# -*- coding: utf-8 -*-
"""IDMPCD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vq6T4Xjaw8f37Cil4BV-0N59hzpfuD2p

## **Test Phase of IDMPCD**

Tabular Data
"""

#!pip install tensorflow-gpu

from google.colab import drive
drive.mount('/content/drive')

"""# **Importing Libraries**"""

## importing Tensorflow 
import tensorflow as tf
print(tf.__version__)

# For importing Data I am using Pandas Python Library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""# **Importing Dataset**
I have uploaded dataset on my Gdrive and importing it using Pandas.
"""

# Importing dataset from Gdrive
cad_data = pd.read_csv('/content/heart1.xlsx - heart.csv')
# Displaying Imported Dataset
cad_data

"""# **Data Preprocessing**"""

# Displaying Some information about the data
cad_data.info()

# Displaying missing values in the data
cad_data.isnull().sum()

"""We can see that, there are no missing values in the dataset, So we don't need apply any missing value technique."""

# Printing some statistical insights of data
cad_data.describe()

# Checking Skewness of data columns, Acceptable values of skewness fall between − 3 and + 3.
cad_data.skew()

"""We can see all columns of our dataset are normaly skewed."""

# Checking Kurtosis of dataset, kurtosis is appropriate from a range of − 10 to + 10
cad_data.kurtosis()

# Checking balance of target variable
cad_data['target'].value_counts()

"""We can see that from our target variable that our dataset is almost evenly distributed
*   1 Represents: CAD present
*   0 Represents: CAD not present
"""

cad_data

"""# **Feature Extraction**"""

# Splitting data into features and target
# X contain featrues and Y contain the target variable
X = cad_data.drop(columns='target', axis=1) ## axis = 1 means we only want to drop column, to drop row we'll write axis = 0
Y = cad_data['target']

# Printing Selected Features
print(X)

#Printing Target Variable.
print(Y)

"""## Splitting data into Training and Test set

"""

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=0)

#feature Scaling

from sklearn.preprocessing import StandardScaler
sc =StandardScaler()
X_train = sc.fit_transform(X_train) ## Why to fit_transform in training dataset only? Ans: To avoid data leakage, reference to krish naik video
X_test = sc.transform(X_test)

X_train

X_test

X_train.shape

X_test.shape

"""## Creating the ANN"""

## Sequential means a huge block, inside contains Neural network, in which we can do complete forward
## propagation and backward propagation
from tensorflow.keras.models import Sequential
## Dense layer is used to create the neurons e.g input layer, hidden layer, output layer
from tensorflow.keras.layers import Dense
## Activation functio is used inside the hidden layer to activate the neurons
from tensorflow.keras.layers import LeakyReLU,PReLU,ELU,ReLU
## When Training accuracy is high and test accuracy score is low, then we dropout some neurons in hidden layers similar to Regularization in ML
from tensorflow.keras.layers import Dropout

## Initializing the ANN
classifier = Sequential()

## Adding the input layer
classifier.add(Dense(units=13, activation='relu'))

## Adding the first hidden layer
classifier.add(Dense(units=7, activation='relu'))

## Adding the second hidden layer
classifier.add(Dense(units=6, activation='relu'))

## Adding the output layer
classifier.add(Dense(1,activation='sigmoid'))

## Compiling the ANN
classifier.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])

"""## Training Model"""

## Early Stopping: when the accuracy of the model is not increasing, training of the model will automatically stop.
import tensorflow as tf

earlyStop = tf.keras.callbacks.EarlyStopping(
              monitor="val_loss",
              min_delta=0.0001, ## Minimum change in the monitored quantity to qualify as an improvement
              patience=20, ## Number of epochs with no improvement after which training will be stopped.
              verbose=1, ## Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1 displays messages when the callback takes an action.
              mode="auto", 
              baseline=None,
              restore_best_weights=False)

model_history = classifier.fit(X_train, y_train, validation_split = 0.33, batch_size=10, epochs = 1000, callbacks= earlyStop)

## To see parameters on which we are focused on during model training.
model_history.history.keys()

## Summarized history of accuracy
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc = 'upper left')
plt.show()

## Summarized history of loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc = 'upper left')
plt.show()

## Making Prediction and evaluating the model

# Predicting the test set results
yPred = classifier.predict(X_test)
yPred = (yPred >= 0.5)

## Calculating the accuracy

from sklearn.metrics import accuracy_score
score = accuracy_score(yPred, y_test)

score

"""## Saving the Model"""



classifier.save('tb_mdl.h5')

ld_mdl = tf.keras.models.load_model('tb_mdl.h5')